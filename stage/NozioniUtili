1. prima distances e poi clustering creo sottoinsiemi: vengono messe assieme fiori della stessa specie

2. per evitare di fare troppo overfitting: creare un modello sui dati di addestramento tenstandolo cosi su insieme diverso, facendolo inoltre piu' volte: cross validatio -> test e score: mi da' la precisione media

3. curca roc: rapporto fra sensibilita'(veri positivi) e 1-specificita' (falsi positivi). La curva ROC è quindi il tasso dei veri positivi in funzione del tasso dei falsi positivi. In generale. Mi dice quindi quanto buono e' il metodo: piu' alto e' auc e migliore e' il metodo.
In generale:
- predizione: positivo e valore reale: positivo -> vero positivo
- predizione: negativo e valore reale: positivo -> falso negativo
- predizione: positivo e valore reale: negativo -> falso positivo
- predizione: negativo e valore reale: negativo -> vero negativo

4. La validazione incrociata (cross-validation), in particolare k-fold, è una tecnica che consiste nella suddivisione dei dati in k insiemi di uguale numerosità. Ad a ogni passo, la k-esima parte dell'insieme di dati viene usta per il test , mentre i altri insiemi per l'addestramento (1->test + 9->addestramento = 10). Così si allena il modello per ognuna delle k parti, evitando quindi problemi di sovradattamento, ma anche di campionamento asimmetrico (e quindi affetto da distorsione) del campione osservato, tipico della suddivisione dei dati in due sole parti (ossia addestramento/convalida). In altre parole, si suddivide il campione osservato in gruppi di egual numerosità, si cambianda ogni volta l'insieme k per test, e si cerca di predirlo coi gruppi non esclusi, al fine di verificare la bontà del modello di predizione utilizzato.

5.PCA: usato quando ho database con molti dati: non e' detto che infatti tutti mi servano, molti potrebbero raccontare infatti la stessa cosa. Ecco quindi che PCA mi permette di capire se posso aggregare fra loro misure, eliminando le dimensioni inutili. Nella pratica sfrutta il fatto che i punti possono essere trasfotmati in vettori. Ciò avviene tramite una trasformazione lineare delle variabili che proietta quelle originarie in un nuovo sistema cartesiano in cui la nuova variabile con la maggiore varianza viene proiettata sul primo asse, la variabile nuova, seconda per dimensione della varianza, sul secondo asse e così via. La riduzione della complessità avviene limitandosi ad analizzare le principali, per varianza, tra le nuove variabili.

Diamo quindi una copertura ai dati pari al 80%, cosi da ricavarne le componenti piu' significative. Su ciascuna componente si possono fare tutta una serie di consideraioni in base alla sua composizione (es. dataset wine dove per ogni componente si mostra la presenza in % di ogni sostanza -> PC1 vede una predominanza di flavonoidi)

6. La media è il rapporto tra la somma dei dati numerici ed il numero dei dati; la moda è il valore che si presenta con maggiore frequenza; la mediana è il valore centrale tra i dati numerici.

7.Rank: mi permette di vedere quale classe/funzionalita' e' piu' significativa dentro un dataset (maggiormene correlate), grazie all'attribuzione di un pinteggio

8. Ogni problema di data mining descrive i dati con delle funzionalita' rendendo cosi il punteggio che a ciascuna di queste viene attribuita una caratteristica fondamentale

9. k-Means funziona bene su dati di forma compatta ma fallisce su distribuzioni differenti (magari ne individua 2 grappoli invece di 1)

10. Il valore della silhouette è una misura di quanto un oggetto sia simile al suo cluster (coesione) rispetto ad altri cluster (separazione). La silhouette varia da −1 a +1, dove un valore elevato indica che l'oggetto è ben abbinato al proprio cluster e scarsamente abbinato ai cluster vicini. Se la maggior parte degli oggetti ha un valore elevato, la configurazione del clustering è appropriata. Se molti punti hanno un valore basso o negativo, la configurazione del cluster può avere troppi o troppo pochi cluster.
La silhouette può essere calcolata con qualsiasi metrica di distanza , come la distanza euclidea o la distanza di Manhattan.

11. Mi da' l'indicazione di quanto e' coeso il punto in esame rispetto al cluster. Range fra 0-1: piu' e' grande piu' il punto e' immerso nel cluster, piu' e' basso piu' e' vicino ad un altro cluster.

12. Nella probabilità conoscendo il processo di generazione dei dati sperimentali (modello probabilistico), siamo in grado di valutare la probabilità dei diversi possibili risultati di un esperimento.  Nella statistica invece il processo di generazione dei dati sperimentali non è noto in modo completo (il processo in questione è, in definitiva, l'oggetto di indagine) e le tecniche statistiche si prefiggono di indurre le caratteristiche di tale processo sulla base dell'osservazione dei dati sperimentali da esso generati. --> non si sa come si genrano i risultati: osservo

13. VC-dimension: cardinalità dell'insieme più grande frantumabile da F => in generale per uno spazio a k dimensioni VC(H)=k+1.
La dimensione VC per un classificatore lineare è almeno 3 perche' non si riesce a frammentare 4 punti.
La VC-dimension viene usata per classificare i diversi tipi di algoritmi in base alla loro complessità. La complessità di un algoritmo di classificazione, che è direttamente correlata alla sua dimensione VC, è correlata al compromesso tra distorsione e varianza. 
All'aumentare della complessità, si passa dal sottoadattamento a quello eccessivo; l'aggiunta di complessità è valida fino a un certo punto, dopo di che si inizia a sovraccaricare i dati di train. Un altro modo di pensare a questo è attraverso baias e la varianza. Un modello a bassa complessità sebbene abbia un basso potere espressivo è anche molto semplice, quindi ha prestazioni molto prevedibili che portano a una bassa varianza. Al contrario, un modello complesso avrà un baias inferiore poiché ha una maggiore espressività, ma avrà una varianza maggiore in quanto vi sono più parametri da ottimizzare in base ai dati di addestramento del campione.
In generale, un modello con una dimensione VC più elevata richiederà un numero maggiore di dati di training per l'addestramento corretto, ma sarà in grado di identificare relazioni più complesse nei dati.
Ad un certo livello di complessità del modello esisterà un equilibrio ideale tra distorsione e varianza, in corrispondenza della quale non si è né insufficienti né adatti ai propri dati. A questo punto si dovrebbe mirare a scegliere un classificatore con un livello di complessità che è appena sufficiente per il  compito di classificazione.
"La macchina con la capacità più piccola (dimensione VC)
è la migliore." Da notare che qui non va intesa la "capacita' min" come "semplicita'" nel n. di parametri (infatti vi possono essere modelli con tanti parametri che hanno una bassa VC e viceversa), ma come semplicita' dell'agotitmo stesso.

14. Margine: è definito come la distanza tra i vettori di supporto di due classi differenti più vicini all’iperpiano. Alla metà di questa distanza viene tracciato l’iperpiano, o retta nel caso si stia lavorando a due dimensioni.

15. SVM ha l’obiettivo di identificare l’iperpiano che meglio divide i vettori di supporto in classi
NB: Se prendo punti linearmente dipendenti non posso sperare di frantumarli, quindi in genere non li considero.

16. Nasce dal fatto che la bonta' di un modello si valuta in base alla sua capacita' di predire l'ignoto. Ecco che usiamo metodo di ricamionamento: Cross validation. Ciascun modello che ottengo (che poi verra' buttato), mi fornisce un errore; per trovare la bonta' del modello io faccio la media di questi errori per trovare la diffusione di giusto/sbagliato del mio modello.

